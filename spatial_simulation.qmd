---
title: "Spatial Causal Inference"
author: Jiaxi Wu
format:
  html:
    # embed-resources: true
    self-contained: true
    code-fold: true
    code-summary: "Show the code"
editor: visual
---

```{r, include = FALSE}
setwd("/Users/jiaxi/research/spatial-causal")
```

```{r, message = FALSE}
library(cmdstanr)
library(tidyverse)
library(INLA)
library(fmesher)
# library(sensemakr)
library(randomForest) 
library(dml.sensemakr)
# library(rstan)
# rstan_options(auto_write = TRUE)
source("utilities.R")
options(mc.cores=parallel::detectCores())
```

# General omitted variable bias

We first consider the outcome model which is linear in $T$ [(Chernozhukov et al., 2022)](https://www.nber.org/system/files/working_papers/w30302/w30302.pdf): $Y = \beta T + f(X,S,U) + \epsilon$. Since $U$ is unobserved, the "naive" regression with observed variables is $Y = \tilde{\beta}T + \tilde{f}(X,S) + \tilde{\epsilon}$. Then the confounding bias is characterized by

```{=tex}
\begin{equation}
    |\tilde{\beta} - \beta|^2 =\rho^2B^2\leq B^2 = S^2C_Y^2C_T^2,
\end{equation}
```
where $\rho^2 = Cor^2(g-\tilde{g}, \alpha-\tilde{\alpha})$, $B^2=E(g - \tilde{g})^2E(\alpha - \tilde{\alpha})^2$, $S^2 = E(Y-\tilde{g})^2E\tilde{\alpha}^2$ is identifiable, $C_Y^2 = R^2_{Y-\tilde{g}\sim g-\tilde{g}} = \eta_{Y\sim U\mid T,X,S}^2$, $C_T^2 = \frac{1-R^2_{\alpha \sim \tilde{\alpha}}}{R^2_{\alpha \sim \tilde{\alpha}}} = \frac{\eta_{T\sim U \mid X,S}^2}{1-\eta_{T\sim U\mid X,S}^2}$. There are three unidentified sensitivity parameters $\rho^2$, $\eta_{T\sim U \mid X,S}^2$ and $\eta_{Y\sim U|T,X,S}^2$.

Slides on [causal inference with machine learning](https://congress-files.s3.amazonaws.com/2022-09/Using%20Machine%20Learning%20for%20Causal%20Inference%20in%20Economics.pdf).

# Partially linear in $U$

For the DGP which is linear in $U$:

```{=tex}
\begin{align} 
    U &= g_U(X,S) + V, \\
    T &= \alpha_UU + g_T(X,S) + \delta, \\
    Y &= g_Y(T, X,S)+ \beta_UU  + \epsilon,
\end{align}
```
Spatial modeling usually considers a partially linear model with spatial patterns modeled by Gaussian processes, splines or CAR processes:

```{=tex}
\begin{align} 
    T &= \alpha_XX + \alpha_UU + Z + \delta, \\
    Y &= \beta_TT + \beta_XX + \beta_UU + W + \epsilon, 
\end{align}
```
where $Z$ and $W$ are spatial variables, but $U$ is not a fixed measurable function of $S,X$ so it is insufficient to adjust for $S,X$ only. We also assume $U = f(S) + V$, $V \sim N(0,\sigma^2)$, $\delta \sim N(0,\sigma_T^2)$, $\epsilon \sim N(0,\sigma_Y^2)$. The exogeneity variables are independent of all the other variables.

The bias can be characterized by $\eta_{Y\sim U|T,X,S}^2$ and $\eta_{T\sim U|X,S}^2$ without $\rho^2$. The partially linear model $Y_i=\beta T_i +g(S_i)+\epsilon_i$ can be identified under some smoothness assumptions ([Gilbert et al. 2023](https://arxiv.org/pdf/2308.12181.pdf)).

```{=tex}
\begin{equation}
    |\tilde{\beta}_T - \beta_T|^2 = E(Y-\tilde{g})^2E\tilde{\alpha}^2 \eta_{Y\sim U|T,X,S}^2 \frac{\eta_{T\sim U|X,S}^2}{1-\eta_{T\sim U|X,S}^2} = \frac{\tilde{\sigma}_Y^2}{\tilde{\sigma}_T^2}\frac{\eta_{Y\sim U|T,X,S}^2\eta_{T\sim U|X,S}^2}{1-\eta_{T\sim U|X,S}^2}.
\end{equation}
```


# Spatial location only affects the unmeasured confounder

Suppose spatial location only affects potential outcomes and exposure through the unmeasured confounders, we focus on the partially linear DGP:

```{=tex}
\begin{align} 
    U &= f(S,X) + V, \\
    T &= \alpha_XX + \alpha_UU + \delta, \\
    Y &= \beta_TT + \beta_XX + \beta_UU + \epsilon.
\end{align}
```
If we fit $T \sim \alpha_XX + \tilde{f}(S) + \tilde{\delta}$ and $Y \sim \tilde{\beta}_TT + \tilde{\beta}_XX + \tilde{\beta}_U\tilde{f}(S) + \tilde{\epsilon}$ on observed data, then the confounding bias can be characterized by a single sensitivity parameter $\eta^2_{T\sim U \mid X,S}$:

```{=tex}
\begin{equation}
\tilde{\beta}_T - \beta_T = \beta_Uw = \tilde{\beta}_U\frac{\eta^2_{T\sim U \mid X,S}}{1 - \eta^2_{T\sim U \mid X,S}}.
\end{equation}
```
This result can be generalized to models that are nonlinear in $T$ and $X$.

```{r, eval = FALSE}
# generate data, linear model
set.seed(62323)
n <- 100 
s <- matrix(runif(2*n, -1, 1), ncol = 2)
u <- gp_sample(s, a2 = 1, l = 1, sigma2 = 0.2) # n by 1, sigma2 = 0.2 or 0.5?
# sigma <- sqrt(0.5)
# u <- sin(2*pi*s[,1]*s[,2]) + s[,1] + s[,2] + rnorm(n, sd = sigma)
sigma_t <- sqrt(0.2)
t <- u + rnorm(n, sd = sigma_t)
sigma_y <- sqrt(0.2)
y <- t + u + rnorm(n, sd = sigma_y)

data_list <- list(D=ncol(s), N=nrow(y), s=s, t=t[,1], y=y[,1])

# first version: fit model, 1-4 correspond to sigma2 0.1/0.2/0.3/chisq prior
# prior matters, dont mix identifiable and unidentifiable parameters
# separate them by transparent parameterization
# try uniform(0,1) and beta(2,2) prior for eta2 as well

sm <- cmdstanr::cmdstan_model("gp_linear_u.stan")
run_sampler <- 1
if(run_sampler) {
  stan_results <- sm$sample(data_list, iter_warmup = 3000, iter_sampling = 2000,
                          refresh = 500)
  stan_results$save_object(file="simulation_linear_eta2_25.RDS")
} else {
  stan_results <- readRDS("simulation_linear_eta2_25.RDS")
}

stan_results$summary(variables = c("rho",	"alpha", "b_t", "b_u", "sigma_t",
                                   "sigma_y"))
draws_df <- stan_results$draws(variables = c("b_t"), format = "df")

# marginalize out u
sm <- cmdstanr::cmdstan_model("linear_transparent.stan")
stan_results <- sm$sample(data_list, iter_warmup = 2000, iter_sampling = 2000,
                          refresh = 200)
stan_results$summary(variables = c("rho",	"alpha", "b_t", "b_u", "sigma_t", "sigma_y", "eta2", "beta"))
draws_df <- stan_results$draws(variables = c("eta2", "beta"), format = "df")
colnames(draws_df)[2] <- "b_t"
```

# Variables at different resolutions

If $T_i$ is invariant at location $i$, the causal effect is not directly identified, but we may use $Y_{ij}-Y_{ik}$ or the outcome model to estimate $\sigma_Y^2$. Then $\eta_{Y\sim U|T,X,S}^2 = \frac{E(g - \tilde{g})^2}{E(Y - \tilde{g})^2} = \frac{E(g - \tilde{g})^2}{E(g - \tilde{g})^2 + \sigma_Y^2}$ is identified from $E(Y - \tilde{g})^2$ and $\sigma_Y^2$.

We omit covariates $X_{ij}$ for brevity or imagine they are absorbed by the functions of $S_i$. Consider the following data generating process with replications of observations at each location:

```{=tex}
\begin{align}
    U_i &= f(S_i) + V_i, \\
    T_i &= \alpha_UU_i + Z_i + \delta_i, \\
    Y_{ij} &= \beta_TT_i + \beta_UU_i + W_i + \epsilon_{ij}, 
\end{align} 
```
where $Y_{ij}$ denotes the $j$th observation at location $i$, $Z_i=f_Z(S_i)$ and $W_i=f_W(S_i)$ are fixed, measurable functions of $S_i$, $V_i \sim N(0,\sigma^2)$, $\delta_i \sim N(0,\sigma_T^2)$, $\epsilon_i \sim N(0,\sigma_Y^2)$ are independent of each other and $S_i$. The estimand is $E[Y_{ij}(t+1)] - E[Y_{ij}(t)] = E[Y_i(t+1)\mid S_i=s] - E[Y_i(t)\mid S_i=s] = \beta_T$. Let $Y$ denote the vector of outcomes for all observations.

::: {.callout-note icon="false"}
## Bias of $\tilde{\beta}_T$

Fit models on observed data $T_i\mid S_i \overset{\mathrm{iid}}{\sim} N(\tilde{f}(S_i), \tilde{\sigma}_T^2)$ and $Y\mid T,S \sim N(\tilde{\beta}_TT + \tilde{h}(S),\Sigma)$ with GP priors on $\tilde{f}$ and $\tilde{h}$. $\Sigma$ follows the block diagonal structure with $Var(Y_{ij} \mid T_i,S_i) = \tilde{\sigma}_Y^2$ and $Cov(Y_{ij},Y_{ik}\mid T, S) = \tilde{\sigma}_Y^2 - \sigma_Y^2$. Then $\tilde{\sigma}_T^2$, $\tilde{\sigma}_Y^2$ and $\sigma_Y^2$ are identifiable and the bias is:

$$
|\tilde{\beta}_T - \beta_T|^2 = \frac{\tilde{\sigma}_Y^2 - \sigma_Y^2}{\tilde{\sigma}_T^2}\frac{\eta_{T\sim U \mid X,S}^2}{1-\eta_{T\sim U\mid X,S}^2}
$$
:::

Note that $\tilde{h}(S)$ are repeated measurements, so the GP prior should be assigned to non-repeated locations.

Another way of recovering $\sigma^2_Y$ is to fit a mixed effect model $Y_{ij} = \tilde{\beta}_TT_i + \tilde{h}(S_i) + \tilde{V}_i + \epsilon_{ij}$.

# Simulation 

Let $T$ denote the treatment, $Y$ denote the outcome, $S$ denote the spatial coordinates, $U$ denote the unobserved spatial confounders, and $X$ denote the observed covariates. In this simulation we generate data from the following data generating process:

```{=tex}
\begin{align}
    U_i &= f(S_i) + V_i, \\
    T_i &= \alpha_UU_i + Z_i + \delta_i, \\
    \mu_i &= \beta_TT_i + \beta_UU_i + W_i\\
    Y_{ij} &= \mu_i + \epsilon_{ij}, 
\end{align} 
```
where $Y_{ij}$ denotes the $j$th observation at location $i$, $Z_i=f_Z(S_i)$ and $W_i=f_W(S_i)$ are fixed, measurable functions of $S_i$, $V_i \sim N(0,\sigma^2)$, $\delta_i \sim N(0,\sigma_T^2)$, $\epsilon_{ij} \sim N(0,\sigma_Y^2)$ are independent of each other and $S_i$. The estimand is $E[\mu_i(t+1)] - E[\mu_i(t)] = E[Y_{ij}(t+1)] - E[Y_{ij}(t)] = \beta_T$.

In the simulation, we generate data with $\alpha_U = \beta_T = \beta_U =1, \sigma^2=\sigma_T^2=\sigma_Y^2=0.2$. So the true values of parameters should be $w=\eta_{T\sim U\mid X,S}^2 = 0.5, \eta_{Y\sim U\mid T,X,S}^2 =1/3, \tilde{\sigma}_T^2 = 0.4, \tilde{\sigma}_Y^2 = 0.3, \tilde{\beta}_T = 1.5$. For $Y$ model, the extreme robustness value is $0.75$, robustness value should be $0.8$. For $\mu$ model, the extreme robustness value is $0.9$. $f(S)$, $Z$ and $W$ are generated from GP. We sample $2000$ observations at $500$ locations.

```{r}
set.seed(111)
# randomly generate location indices?
m <- 500 # number of locations
n <- 2000
s <- matrix(runif(2*m, -1, 1), ncol = 2) # rep(s[,1], times=a)
group <- rep(1:m, each=n/m)
u <- gp_sample(s, a2 = 1, l = 1, sigma2 = 0.2) 
z <- gp_sample(s, a2 = 1, l = 2, sigma2 = 0.2)
w <- gp_sample(s, a2 = 1, l = 3, sigma2 = 0)
t <- u + z # + rnorm(m, mean = 0, sd = sqrt(0.2))  
mu <- t + u + w
u_rep <- u[rep(1:nrow(u), each=n/m),] 
t_rep <- t[rep(1:nrow(t), each=n/m),] 
w_rep <- w[rep(1:nrow(w), each=n/m),] 
s_rep <- s[rep(1:nrow(s), each=n/m),] 
y <- t_rep + u_rep + w_rep + rnorm(n, mean = 0, sd = sqrt(0.2)) 
mu_est <- colMeans(matrix(y, nrow=n/m))
```

## Aggregate-level modeling

In the first approach, we partial out $S$, then get robustness values using *sensemakr*. When use $\bar{Y}_i$ as outcome, residual outcome variance is all explained by $U$, $\eta^2_{Y\sim U|T,S} = 1$.

```{r}
# DML with Random Forest:
t_reg <- function(x, t){randomForest(x, t)} 
y_reg <- function(x, y){randomForest(x, y)} 

# partial out S
set.seed(111)
# use other folds to fit model and predict on the remaining fold, not tune hyperparameters
dml_rf <- dml_plm(s, t, mu_est, t_reg, y_reg, nfold=10) # CV by location, not by replicated units 
print(c("Coefficeint estimate", dml_rf$coef.est))
resY <- dml_rf$y_til
resT <- dml_rf$t_til

# resT <- get_residuals(s, t, t_reg, nfold=10)
# resY1 <- get_residuals(cbind(s,t), mu_est, y_reg, nfold=10)
# print(c("Controls explain the following fraction of variance of Treatment",
#         max(1 - var(resT)/var(t), 0)))
# print(c("Controls explain the following fraction of variance of Outcome", 
#         max(1 - var(resY)/var(y), 0)))
# summary(lm(resY[(1:m) * (n/m)] ~ resT))
# resT_rep <- resT[rep(1:length(resT), each=n/m)]
res_model <- lm(resY ~ resT) # robustness values calculated from var(resT) and var(res_model$residuals)
print(c("Treatment residual variance:", var(resT))) # should be 0.4
print(c("Outcome residual variance:", var(res_model$residuals)))

dml_sensitivity <- sensemakr(model = res_model, treatment = "resT")
summary(dml_sensitivity)
plot(dml_sensitivity, nlevels = 15)

etaT2 <- 0.5
result_df <- data.frame(method = "partial out", 
                        estimate = as.numeric(res_model$coefficients[2]),
                        lower = as.numeric(res_model$coefficients[2]) - 
                          sqrt(var(res_model$residuals) / var(resT) * etaT2 / (1 - etaT2)),
                        upper = as.numeric(res_model$coefficients[2]) + 
                          sqrt(var(res_model$residuals) / var(resT) * etaT2 / (1 - etaT2))
                        )
result_df
```
The naive estimate $\tilde{\beta}_T=1.5$ is recovered. But estimate of treatment residual variance and outcome residual variance are not accurate.

Second, we use *dml.sensemakr*. *format.perc* should be replaced by *format_perc* when calculating confidence bounds based on asymptotic normality.

```{r}
# x <- model.matrix(~ -1 + t + s, data = data_agg)
dml_fit <- dml(mu_est, as.vector(t), s, model = "plm", cf.folds = 5, cf.reps = 5)
summary(dml_fit)
dml_bounds(dml_fit, cf.y = 1, cf.d = 0.5)
result_df <- rbind(result_df, c("dml", 1.496, 0.871, 2.113))
```

Fit treatment and outcome models and estimate $\tilde{\sigma}_T^2$ by optimization using stan:

```{r}
# stan
data_list_t <- list(D=ncol(s), N=m, s=s, t=t[,1])
trt_model <- cmdstan_model(stan_file = "stan/treatment_gp.stan")
trt_opt <- trt_model$optimize(data=data_list_t, init=2, refresh=100, iter=1000)
opt_draws <- trt_opt$draws()
sigmaT_opt <- as.numeric(subset(opt_draws, variable=c('sigma_t')))^2
cat("MAP of sigma_t_tilde_sq =", sigmaT_opt)

# estimate sigma_y_tilde
data_list_y <- list(D=ncol(s), N=length(mu_est), s=s, t=t[,1], y=mu_est)
outcome_model <- cmdstan_model(stan_file = "stan/outcome_gp.stan")
outcome_opt <- outcome_model$optimize(data=data_list_y, init=2, refresh=100, iter=1000)
outcome_opt_draws <- outcome_opt$draws()
print(subset(outcome_opt_draws, variable=c('beta', 'sq_sigma_y')))
beta <- as.numeric(subset(outcome_opt_draws, variable=c('beta')))
bias <- as.numeric(subset(outcome_opt_draws, variable=c('sq_sigma_y'))) / sigmaT_opt * 1 *
  etaT2 / (1 - etaT2)
result_df <- rbind(result_df, c("agg stan", beta, beta - bias, beta + bias))
result_df
```

Fit treatment and outcome models by INLA:

```{r}
# treatment model
# use fmesher to create meshes, tradeoff between accuracy and computational cost
mesh1 <- fm_mesh_2d_inla(loc=s, max.edge=c(0.05, 0.15), cutoff=0.02, offset=c(0.1,0.4))
plot(mesh1)
points(s, pch=3, bg=1, col="red", cex=1)

# create projector matrix A, mapping triangulation vertices on mesh to observation locations
A.est <- inla.spde.make.A(mesh=mesh1, loc=s)
print(dim(A.est)) # number of data locations * number of mesh nodes

# fit model
spde <- inla.spde2.matern(mesh=mesh1, alpha=2) 
formula <- y ~ -1 + intercept + f(spatial.field, model=spde)
# spatial.field is a index variable?
result <- inla(formula, 
               data = list(y=t[,1], intercept=rep(1, spde$n.spde),
               spatial.field=1:spde$n.spde),
               control.predictor=list(A=A.est, compute=TRUE))

# round(result$summary.fixed, 3)
print(round(result$summary.hyperpar, 3))
sigmaT2_inla <- inla.emarginal(function(x) 1/x, result$marginals.hyper[[1]])
cat("posterior mean of sigma_t_tilde_sq =", sigmaT2_inla) # posterior mean of variance

# outcome model
mesh.index <- inla.spde.make.index(name = "field", n.spde = spde$n.spde,
                                   n.repl = 1)
# stack the predictor information
stack <- inla.stack(data = list(y = mu_est), A = list(A.est, 1),
                    effects = list(c(mesh.index, list(intercept = 1)),
                                   list(cov = t[,1])), tag = "est")
# fit model
formula <- y ~ -1 + intercept + cov + f(field, model = spde) 
inla_result <- inla(formula, data = inla.stack.data(stack, spde = spde),
                   family = "normal",
                   control.predictor = list(A = inla.stack.A(stack),
                                             compute = TRUE))
print(round(inla_result$summary.fixed, 3))
print(round(inla_result$summary.hyperpar, 3))
sigmaY2_inla <- inla.emarginal(function(x) 1/x, inla_result$marginals.hyper[[1]])
cat("posterior mean of sigma_y_tilde_sq =", sigmaY2_inla) 
beta <- inla_result$summary.fixed$mean[2]
bias <- sqrt(sigmaY2_inla / sigmaT2_inla * 1 * etaT2 / (1 - etaT2))

result_df <- rbind(result_df, c("agg inla", beta, beta - bias, beta + bias))
```


## Individual-level modeling 

Partial out $S$ directly, but avoid splitting replications into different folds in cross validation.

```{r}

```

We check if $\tilde{\beta}_T$, $\tilde{\sigma}_T^2$, $\tilde{\sigma}_Y^2$ and $\sigma_Y^2$ can be consistently estimated by fitting models $T_i\mid S_i \overset{\mathrm{iid}}{\sim} N(\tilde{f}(S_i), \tilde{\sigma}_T^2)$ and $Y\mid T,S \sim N(\tilde{\beta}_TT + \tilde{h}(S),\Sigma)$ with group structure using stan. 

```{r mcmc, message=FALSE}
# set.seed(123)
# m <- 100 # number of locations
# n <- 300
# s <- matrix(runif(2*m, -1, 1), ncol = 2) # rep(s[,1], times=a)
# u <- gp_sample(s, a2 = 1, l = 1, sigma2 = 0.2) 
# z <- gp_sample(s, a2 = 1, l = 2, sigma2 = 0.2)
# w <- gp_sample(s, a2 = 1, l = 3, sigma2 = 0)
# t <- u + z # + rnorm(m, mean = 0, sd = sqrt(0.2))  
# # s <- s[rep(1:nrow(s), each=n/m),] 
# u <- u[rep(1:nrow(u), each=n/m),] 
# t <- t[rep(1:nrow(t), each=n/m),] 
# w <- w[rep(1:nrow(w), each=n/m),] 
# # s <- do.call(rbind, replicate(5, s, simplify=FALSE))
# # u <- do.call(rbind, replicate(5, u, simplify=FALSE))
# # t <- do.call(rbind, replicate(5, t, simplify=FALSE))
# y <- t + u + w + rnorm(n, mean = 0, sd = sqrt(0.2)) # + w

# plot
# u_density <- tibble(f = u, s1 = s[,1], s2 = s[,2])
# u_density %>%
#   ggplot(aes(x=s1, y=s2, color=f)) +
#   geom_point(alpha=1, size=2.5) +
#   theme_bw()

# MAP estimate of sigma_y_tilde and sigma_y
data_list <- list(D=ncol(s), M=m, N=length(y), s=s, t=t_rep, y=y)
outcome_model_stan <- cmdstan_model(stan_file = "stan/replication_t_u.stan")
outcome_opt <- outcome_model_stan$optimize(data=data_list, init=2, refresh=100, iter=1000)
outcome_opt_draws <- outcome_opt$draws()
print(subset(outcome_opt_draws, variable=c('beta', 'sq_sigma_y', 'sq_sigma_y_tilde')))
beta_opt <- as.numeric(subset(outcome_opt_draws, variable=c('beta')))
sigmaY2_opt <- as.numeric(subset(outcome_opt_draws, variable=c('sq_sigma_y')))
sigmaY2_tilde_opt <- as.numeric(subset(outcome_opt_draws, variable=c('sq_sigma_y_tilde')))

result_df <- 
  rbind(result_df, c("ind stan", beta_opt,
                     beta_opt - sqrt((sigmaY2_tilde_opt - sigmaY2_opt) / sigmaT_opt * 1),
                     beta_opt + sqrt((sigmaY2_tilde_opt - sigmaY2_opt) / sigmaT_opt * 1)))

# mcmc 
run_sampler <- 0
if(run_sampler) {
  stan_results <- outcome_model_stan$sample(data_list, iter_warmup = 1000, 
                                         iter_sampling = 1000, refresh = 500) 
  stan_results$save_object(file="simulation_rep_n300.RDS")
} else {
  stan_results <- readRDS("simulation_rep_n300.RDS")
}
result <- stan_results$summary(variables = c('beta', 'sq_sigma_y', 'sq_sigma_y_tilde'))
print(result)
```

The posterior estimates are close to true values of $\tilde{\sigma}_T^2$, $\tilde{\beta}_T$, $\tilde{\sigma}_Y^2$ and $\sigma_Y^2$, and the $90\%$ credible intervals cover them.

Next we try faster approximate Bayesian inference using [R-INLA](https://www.r-inla.org/home). The continuous domain spatial modeling is done by stochastic partial differential equation (SPDE) approach. We first build a treatment model.

```{r INLA}
# set.seed(123)
# n <- 500 # number of locations
# s <- matrix(runif(2*n, -1, 1), ncol = 2)
# u <- gp_sample(s, a2 = 1, l = 1, sigma2 = 0.2)
# z <- gp_sample(s, a2 = 1, l = 2, sigma2 = 0.2)
# t <- u + z # + rnorm(n, mean = 0, sd = sqrt(0.2))
# data1 <- data.frame(s1 = s[,1], s2 = s[,2], t = t)
# plot
tibble(f = t, s1 = s[,1], s2 = s[,2]) %>%
  ggplot(aes(x=s1, y=s2, color=f)) +
  geom_point(alpha=1, size=2.5) +
  theme_bw()

# use fmesher to create meshes, tradeoff between accuracy and computational cost
mesh1 <- fm_mesh_2d_inla(loc=s, max.edge=c(0.05, 0.15), cutoff=0.02, offset=c(0.1,0.4))
plot(mesh1)
points(s, pch=3, bg=1, col="red", cex=1)

# create projector matrix A, mapping triangulation vertices on mesh to observation locations
A.est <- inla.spde.make.A(mesh=mesh1, loc=s)
print(dim(A.est)) # number of data locations * number of mesh nodes

# fit model
spde <- inla.spde2.matern(mesh=mesh1, alpha=2) 
formula <- y ~ -1 + intercept + f(spatial.field, model=spde)
# spatial.field is a index variable?
result <- inla(formula, 
               data = list(y=t[,1], intercept=rep(1, spde$n.spde),
               spatial.field=1:spde$n.spde),
               control.predictor=list(A=A.est, compute=TRUE))

# round(result$summary.fixed, 3)
print(round(result$summary.hyperpar, 3))
cat("posterior mean of sigma_t_tilde_sq =", inla.emarginal(function(x) 1/x, result$marginals.hyper[[1]])) # posterior mean of variance
```

Posterior mean is close to $0.4$ and $95\%$ credible interval covers true $\tilde{\sigma}_T^2$.

Then we check the posterior distributions of $\tilde{\beta}_T$, $\tilde{\sigma}_Y^2$ and $\sigma_Y^2$ by fitting $Y_{ij} = \beta_0 + \tilde{\beta}_TT_i + \tilde{h}(S_i) + \tilde{V}_i + \epsilon_{ij}$ without marginalizing out $U_i$. The random effect $\tilde{V}_i$ accounts for the between-group variation $\beta_U^2Var(U_i \mid T_i,S_i)= \tilde{\sigma}_Y^2 -\sigma_Y^2 =0.1$.

```{r replication}
# with replicates at each location
# set.seed(111)
# m <- 100 # number of locations
# n <- 500
# s <- matrix(runif(2*m, -1, 1), ncol = 2) # rep(s[,1], times=a)
# group <- rep(1:m, each=n/m)
# u <- gp_sample(s, a2 = 1, l = 1, sigma2 = 0.2) 
# z <- gp_sample(s, a2 = 1, l = 2, sigma2 = 0.2)
# w <- gp_sample(s, a2 = 1, l = 3, sigma2 = 0)
# t <- u + z # + rnorm(m, mean = 0, sd = sqrt(0.2))  
# u <- u[rep(1:nrow(u), each=n/m),] 
# t <- t[rep(1:nrow(t), each=n/m),] 
# w <- w[rep(1:nrow(w), each=n/m),] 
# y <- t + u + w + rnorm(n, mean = 0, sd = sqrt(0.2)) 

mesh <- fm_mesh_2d_inla(loc=s, max.edge=c(0.05, 0.2), offset=c(0.1,0.4))

# For each observation, index gives the corresponding index into the matrix of measurement locations, and repl determines the corresponding replicate index. 
A <- inla.spde.make.A(mesh, loc = s, index = rep(1:m, each = n/m), 
                      repl = rep(1:(n/m), times = m))
print(dim(A))
spde <- inla.spde2.matern(mesh=mesh, alpha=2) 

# index the full mesh and replicates
mesh.index <- inla.spde.make.index(name = "field", n.spde = spde$n.spde,
                                   n.repl = n/m)

# stack the predictor information
stack <- inla.stack(data = list(y = y), A = list(A, 1),
                    effects = list(c(mesh.index, list(intercept = 1)),
                                   list(cov = t_rep, loc = group)), tag = "est")

# fit model
formula <- y ~ -1 + intercept + cov + f(field, model = spde, replicate = field.repl) + f(loc, model = "iid")
rep_result <- inla(formula, data = inla.stack.data(stack, spde = spde),
                   family = "normal",
                   control.predictor = list(A = inla.stack.A(stack),
                                             compute = TRUE))
print(round(rep_result$summary.fixed, 3))
print(round(rep_result$summary.hyperpar, 3))
cat("posterior mean of sigma_y_tilde_sq - sigma_y_sq =", inla.emarginal(function(x) 1/x, rep_result$marginals.hyper[[4]]), '\n')
cat("posterior mean of sigma_y_sq =", inla.emarginal(function(x) 1/x, rep_result$marginals.hyper[[1]])) 

posterior_beta_tilde <- rep_result$marginals.fixed[[2]]
plot(posterior_beta_tilde, type = "l", xlab = 'beta_t_tilde', ylab = 'density',
     main = "Posterior density of beta_t_tilde")

posterior_sigma_y_sq <- rep_result$marginals.hyperpar[[1]]
posterior_sigma_y_sq[,1] <- 1 / posterior_sigma_y_sq[,1]
plot(posterior_sigma_y_sq, type = "l", xlab = 'sigma_y_sq', ylab = 'density',
     main = "Posterior density of sigma_y_sq")

posterior_sigma_y_tilde_sq <- rep_result$marginals.hyperpar[[4]]
posterior_sigma_y_tilde_sq[,1] <- 1 / posterior_sigma_y_tilde_sq[,1] +
  posterior_sigma_y_sq[,1]
plot(posterior_sigma_y_tilde_sq, type = "l", xlab = 'sigma_y_tilde_sq', ylab = 'density',
     main = "Posterior density of sigma_y_tilde_sq")

beta <- round(rep_result$summary.fixed, 3)$mean[2]
bias <- sqrt(inla.emarginal(function(x) 1/x, rep_result$marginals.hyper[[4]]) / sigmaT2_inla * 1)
result_df <- 
  rbind(result_df, c("ind inla", beta, beta - bias, beta + bias))
```

The $95\%$ credible intervals cover true $\tilde{\beta}_T, \tilde{\sigma}_Y^2-\sigma_Y^2, \sigma_Y^2$.

## Plot

```{r}
is.num <- sapply(result_df, is.numeric)
result_df[is.num] <- lapply(result_df[is.num], round, 3)
result_df
```




If not possible to get individual data, try county then hospital level data, may use zipcode

simulation add nonlinear, make different intervals in a figure

find application with individual data? binary outcome for supermarket data? How to justify why sample individual outcomes if we only have aggregate info?

how to do inference? posterior distribution or plug in estimates of identifiable quantities?

try simulation for both settings, negative controls? T_ij help resolve gamma_i

distribution of \bar(Y_i) \sim N(\mu_i, \sigma_Y2/n_i), not exact

# Test sensemakr

By Frisch-Waugh-Lovell Theorem, partialling out covariates does not change coefficient estimates (but changes t test), so *sensemakr* can be used on residuals after partialling out covariates. We check that as long as the outcome model is partially linear in the treatment, we can partial out other covariates and then run linear regression on the residuals.

```{r}
data("darfur")

# runs regression model
model <- lm(peacefactor ~ directlyharmed + (age + farmer_dar + herder_dar +
              pastvoted + hhsize_darfur + female + village)^2, data = darfur)
# runs sensemakr for sensitivity analysis
sensitivity <- sensemakr(model = model, 
                         treatment = "directlyharmed",
                         benchmark_covariates = "female",
                         kd = 1:3)
summary(sensitivity)

# partial out covariates
resY <- lm(peacefactor ~ (age + farmer_dar + herder_dar +
             pastvoted + hhsize_darfur + female + village)^2, data = darfur)$res
resD <- lm(directlyharmed ~ (age + farmer_dar + herder_dar +
             pastvoted + hhsize_darfur + female + village)^2, data = darfur)$res
print(c("Controls explain the following fraction of variance of Outcome", 1-var(resY)/var(darfur$peacefactor)))
print(c("Controls explain the following fraction of variance of Treatment", 1-var(resD)/var(darfur$directlyharmed)))
# sensitivity analysis
res_model <- lm(resY ~ resD) 
res_sensitivity <- sensemakr(model = res_model, treatment = "resD")
summary(res_sensitivity)
```


Test *dml.sensemakr*, does not work.

```{r}
# calc_confint <- function(cf, ses, params=NULL, level) {
#   pnames <- names(ses)
#   if (is.matrix(cf))
#     cf <- setNames(as.vector(cf), pnames)
#   if (is.null(params))
#     params <- pnames
#   else if (is.numeric(params))
#     params <- pnames[params]
#   a <- (1 - level)/2
#   a <- c(a, 1 - a)
#   fac <- qnorm(a)
#   pct <- stats:::format_perc(a, 3)
#   ci <- array(NA_real_, dim = c(length(params), 2L), dimnames = list(params, pct))
#   ci[] <- cf[params] + ses[params] %o% fac
#   ci
# }
# dml.sensemakr::calc_confint <- calc_confint

data("pension")

# set treatment, outcome and covariates
y <- pension$net_tfa  # net total financial assets
d <- pension$e401     # 401K eligibility
x <- model.matrix(~ -1 + age + inc  + educ+ fsize + marr + twoearn + pira + hown, data = pension)

# run plm
dml.401k.plm <- dml(y, d, x, model = "plm", cf.folds = 5, cf.reps = 5)
summary(dml.401k.plm)

sens.401k.plm <- sensemakr(dml.401k.plm, cf.y = 0.04, cf.d = 0.03,)
# bench.plm <- dml_benchmark(dml.401k.plm, benchmark_covariates = c("inc", "pira", "twoearn"))
# bench.plm

# run DML (nonparametric model)
dml.401k <- dml(y, d, x, model = "npm")
summary(dml.401k)
dml_bounds(dml.401k, cf.y = 0.03, cf.d = 0.04)

# sensitivity analysis
sens.401k <- sensemakr(dml.401k, cf.y = 0.03, cf.d = 0.04)
summary(sens.401k)

plot(sens.401k)
```

