---
title: "Spatial Causal Inference"
author: Jiaxi Wu
format:
  html:
    # embed-resources: true
    self-contained: true
    code-fold: true
    code-summary: "Show the code"
editor: visual
---

```{r, include=FALSE}
setwd("/Users/jiaxi/research/spatial-causal")
```

```{r, message = FALSE}
library(cmdstanr)
library(tidyverse)
library(INLA)
library(fmesher)
# library(rstan)
# rstan_options(auto_write = TRUE)
options(mc.cores=parallel::detectCores())
```

```{r functions, message = FALSE}
gp_sample <- function(x, a2 = 1, l = 1, sigma2 = 0.1){
  n <- nrow(x)
  mu <- rep(0, n)
  K <- matrix(nrow = n, ncol = n)
  for (i in 1:(n - 1)) {
    K[i, i] = a2 + sigma2
    for (j in (i + 1):n) {
      K[i, j] = a2 * exp(-sum((x[i,] - x[j,])^2) / l)
      K[j, i] = K[i, j]
    }
  }
  K[n, n] = a2 + sigma2
  return(t(mvtnorm::rmvnorm(1, sigma = K))) 
}
```

## Spatial causal inference setting

Let $T$ denote the treatment, $Y$ denote the outcome, $S$ denote the spatial coordinates, $U$ denote the unobserved spatial confounder, and $X$ denote the observed covariates. $S$ could possibly affect all the other variables and usually $U$ is not a fixed, measurable function of $S$, so the causal effect cannot be identified without additional assumptions if we adjust for $X$ and $S$ only [(Gilbert et al. 2021)](https://arxiv.org/pdf/2112.14946.pdf). In general, we assume exchangeability given $S,X,U$ and positivity. We are interested in estimating the average treatment effect $E[Y(t)]$ and $E[Y(t) \mid X,S]$.

### General omitted variable bias

We first consider the outcome model which is linear in $T$ [(Chernozhukov et al., 2022)](https://www.nber.org/system/files/working_papers/w30302/w30302.pdf): $Y = \beta T + f(X,S,U) + \epsilon$. $\beta$ is equal to the average causal effect or average causal derivative. Since $U$ is unobserved, the "short" regression with observed variables is $Y = \tilde{\beta}T + \tilde{f}(X,S) + \tilde{\epsilon}$. Let $\alpha=\frac{T-E[T|X,S,U]}{E(T-E[T|X,S,U])^2}$, $\tilde{\alpha} =\frac{T-E[T|X,S]}{E(T-E[T|X,S])^2}$, $g=E[Y|T,X,S,U]$, $\tilde{g}=E[Y|T,X,S]$, then $\beta=E[Y\alpha]=E[g\alpha]$, $\tilde{\beta}=E[Y\tilde{\alpha}] =E[\tilde{g}\tilde{\alpha}]$. The omitted variable bias is $\beta - \tilde{\beta} = E[(g - \tilde{g})(\alpha - \tilde{\alpha})]$ and it is bounded by

```{=tex}
\begin{equation}
    |\tilde{\beta} - \beta|^2 =\rho^2B^2\leq B^2,
\end{equation}
```
where $\rho^2 = Cor^2(g-\tilde{g}, \alpha-\tilde{\alpha})$, $B^2=E(g - \tilde{g})^2E(\alpha - \tilde{\alpha})^2$. $B^2$ can be further expressed as

```{=tex}
\begin{equation}
    B^2= S^2C_Y^2C_T^2,
\end{equation}
```
where $S^2 = E(Y-\tilde{g})^2E\tilde{\alpha}^2$ is identifiable, $C_Y^2 = R^2_{Y-\tilde{g}\sim g-\tilde{g}} = \eta_{Y\sim U\mid T,X,S}^2$, $C_T^2 = \frac{1-R^2_{\alpha \sim \tilde{\alpha}}}{R^2_{\alpha \sim \tilde{\alpha}}} = \frac{\eta_{T\sim U \mid X,S}^2}{1-\eta_{T\sim U\mid X,S}^2}$. There are three unidentified parameters $\rho^2$, $\eta_{T\sim U \mid X,S}^2$ and $\eta_{Y\sim U|T,X,S}^2$, which can be benchmarked empirically from observed data.

This result can be extended to fully nonparametric causal models and it only requires the target estimand $\beta:= Em(W,g)$ with $W=(T,X,S,U)$ to be a continuous linear functional of the conditional expectation function of the outcome, $g$. Then there exists unique Riesz representer $\alpha(W)$ such that $\beta= Em(W,g) = Eg(W)\alpha(W)$ and $\tilde{\alpha}(\tilde{W})=E[\alpha(W)\mid \tilde{W}]$ where $\tilde{W} = (T,X,S)$. For example, when the treatment is binary and the estimand is average causal effect $\beta=E[Y(1)-Y(0)]$, we have $\alpha(W) = \frac{1(T=1)}{P(T=1\mid X,S,U)} - \frac{1(T=0)}{P(T=0\mid X,S,U)}$. Another example is average causal derivatives for continuous treatment $\alpha(W) = -\partial_t \log f(T\mid X,S,U)$. 

The OVB is still $\beta - \tilde{\beta} = E[(g - \tilde{g})(\alpha - \tilde{\alpha})]$ and it is bounded by $|\tilde{\beta} - \beta|^2 =\rho^2B^2\leq B^2$. The bound can be characterized by $B^2 = S^2C_Y^2C_T^2$ where $S^2$ and $C_Y^2$ have the same interpretations as above, but $C_T^2 = \frac{1-R^2_{\alpha \sim \tilde{\alpha}}}{R^2_{\alpha \sim \tilde{\alpha}}}$ has different forms under different cases. Take average causal effect with a binary treatment as example, $C_T^2 = \frac{E[1/(P(T=1 \mid X,S,U)P(T=0 \mid X,S,U))]}{E[1/(P(T=1 \mid X,S)P(T=0 \mid X,S))]} - 1$. For average causal derivatives, if $T$ is homoscedastic Gaussian conditional on $X$ and $(X,A)$, then $C_T^2$ simplifies to the one in partially linear model. 

For inference, $S$ and $\tilde{\beta}$ in the bound can be estimated via debiased machine learning (DML), which can eliminate the regularization and overfitting bias. The DML estimators are asymptotically linear and Gaussian, so confidence bounds for the sensitivity bounds have asymptotic coverage.

Slides on [causal inference with machine learning](https://congress-files.s3.amazonaws.com/2022-09/Using%20Machine%20Learning%20for%20Causal%20Inference%20in%20Economics.pdf).

### Partially linear data generating process

For the DGP which is linear in $U$:

```{=tex}
\begin{align} 
    U &= f_U(X,S) + V, \\
    T &= \alpha_UU + f_T(X,S) + \delta, \\
    Y &= f_Y(T, X,S)+ \beta_UU  + \epsilon,
\end{align}
```


Spatial modeling usually considers a partially linear model with spatial patterns modeled by Gaussian processes, splines or CAR processes:

```{=tex}
\begin{align} 
    T &= \alpha_XX + \alpha_UU + Z + \delta, \\
    Y &= \beta_TT + \beta_XX + \beta_UU + W + \epsilon, 
\end{align}
```
where $Z$ and $W$ are spatial variables, but $U$ is not a fixed measurable function of $S,X$ so it is insufficient to adjust for $S,X$ only. We also assume $U = f(S) + V$, $V \sim N(0,\sigma^2)$, $\delta \sim N(0,\sigma_T^2)$, $\epsilon \sim N(0,\sigma_Y^2)$. The exogeneity variables are independent of all the other variables. 

The bias is characterized by

```{=tex}
\begin{equation}
    |\tilde{\beta}_T - \beta_T|^2 = E[(g - \tilde{g})(\alpha - \tilde{\alpha})]^2 = E(g - \tilde{g})^2E(\alpha - \tilde{\alpha})^2 = E(Y-\tilde{g})^2E\tilde{\alpha}^2 \eta_{Y\sim U|T,X,S}^2 \frac{\eta_{T\sim U|X,S}^2}{1-\eta_{T\sim U|X,S}^2}.
\end{equation}
```
The bias can be characterized by $\eta_{Y\sim U|T,X,S}^2$ and $\eta_{T\sim U|X,S}^2$ without $\rho^2$. The partially linear model $Y_i=\beta T_i +g(S_i)+\epsilon_i$ can be identified under some smoothness assumptions ([Gilbert et al. 2023](https://arxiv.org/pdf/2308.12181.pdf)).


### Spatial location only affects the unmeasured confounder

Suppose spatial location only affects potential outcomes and exposure through the unmeasured confounders, we focus on the partially linear DGP:

```{=tex}
\begin{align} 
    U &= f(S,X) + V, \\
    T &= \alpha_XX + \alpha_UU + \delta, \\
    Y &= \beta_TT + \beta_XX + \beta_UU + \epsilon.
\end{align}
```

If we fit $T \sim \alpha_XX + \tilde{f}(S) + \tilde{\delta}$ and $Y \sim \tilde{\beta}_TT + \tilde{\beta}_XX + \tilde{\beta}_U\tilde{f}(S) + \tilde{\epsilon}$ on observed data, then the confounding bias can be characterized by a single sensitivity parameter $\eta^2_{T\sim U \mid X,S}$:

```{=tex}
\begin{equation}
\tilde{\beta}_T - \beta_T = \beta_Uw = \tilde{\beta}_U\frac{\eta^2_{T\sim U \mid X,S}}{1 - \eta^2_{T\sim U \mid X,S}}.
\end{equation}
```
This result can be generalized to models that are nonlinear in $T$ and $X$. 

```{r}
# generate data, linear model
set.seed(62323)
n <- 100 
s <- matrix(runif(2*n, -1, 1), ncol = 2)
u <- gp_sample(s, a2 = 1, l = 1, sigma2 = 0.2) # n by 1, sigma2 = 0.2 or 0.5?
# sigma <- sqrt(0.5)
# u <- sin(2*pi*s[,1]*s[,2]) + s[,1] + s[,2] + rnorm(n, sd = sigma)
sigma_t <- sqrt(0.2)
t <- u + rnorm(n, sd = sigma_t)
sigma_y <- sqrt(0.2)
y <- t + u + rnorm(n, sd = sigma_y)

data_list <- list(D=ncol(s), N=nrow(y), s=s, t=t[,1], y=y[,1])

# first version: fit model, 1-4 correspond to sigma2 0.1/0.2/0.3/chisq prior
# prior matters, dont mix identifiable and unidentifiable parameters
# separate them by transparent parameterization
# try uniform(0,1) and beta(2,2) prior for eta2 as well

sm <- cmdstanr::cmdstan_model("gp_linear_u.stan")
stan_results <- sm$sample(data_list, iter_warmup = 3000, iter_sampling = 2000,
                          refresh = 500)
run_sampler <- 1
if(run_sampler) {
  stan_results <- sm$sample(data_list, iter_warmup = 5000, iter_sampling = 3000,
                          refresh = 500)
  stan_results$save_object(file="simulation_linear_eta2_25.RDS")
} else {
  stan_results <- readRDS("simulation_linear_eta2_25.RDS")
}
# stan_results1$save_object(file="simulation_linear_eta2_1_n200.RDS")

stan_results1$summary(variables = c("rho",	"alpha", "b_t", "b_u", "sigma_t",
                                   "sigma_y"))
stan_results6$summary(variables = c("rho",	"alpha", "b_t", "b_u", "eta2", "sigma_t",
                                   "sigma_y"))
draws_df_1 <- stan_results1$draws(variables = c("b_t"), format = "df")
# draws_df_6 <- stan_results6$draws(variables = c("b_t", "eta2"), format = "df")
# mcmc_hist(draws_df_6)

# marginalize out u
sm <- cmdstanr::cmdstan_model("linear_transparent.stan")
stan_results <- sm$sample(data_list, iter_warmup = 2000, iter_sampling = 2000,
                          refresh = 200)

stan_results$summary(variables = c("rho",	"alpha", "b_t", "b_u", "sigma_t", "sigma_y", "eta2", "beta"))

draws_df <- stan_results$draws(variables = c("eta2", "beta"), format = "df")
colnames(draws_df)[2] <- "b_t"
```

### Replications at each $S$ and variables at different resolutions

If at each $S$, $U$ is invariant, but $T$, $Y$, $X$ are not, then case-control matching methods regress the difference between responses in the same region to remove spatial confounding. If $T_i$ is invariant at location $i$, the causal effect is not directly identified, but we may use $Y_{ij}-Y_{ik}$ or the outcome model to estimate $\sigma_Y^2$. Then $\eta_{Y\sim U|T,X,S}^2 = \frac{E(g - \tilde{g})^2}{E(Y - \tilde{g})^2} = \frac{E(g - \tilde{g})^2}{E(g - \tilde{g})^2 + \sigma_Y^2}$ is identified from $E(Y - \tilde{g})^2$ and $\sigma_Y^2$. This idea works for the general nonparametric models and areal data [(Reich et al., 2020)](https://arxiv.org/pdf/2007.02714.pdf) as well. 

We omit covariates $X_{ij}$ for brevity or imagine they are absorbed by the functions of $S_i$. Consider the following data generating process with replications of observations at each location:

```{=tex}
\begin{align}
    U_i &= f(S_i) + V_i, \\
    T_i &= \alpha_UU_i + Z_i + \delta_i, \\
    Y_{ij} &= \beta_TT_i + \beta_UU_i + W_i + \epsilon_{ij}, 
\end{align} 
```
where $Y_{ij}$ denotes the $j$th observation at location $i$, $Z_i=f_Z(S_i)$ and $W_i=f_W(S_i)$ are fixed, measurable functions of $S_i$, $V_i \sim N(0,\sigma^2)$, $\delta_i \sim N(0,\sigma_T^2)$, $\epsilon_i \sim N(0,\sigma_Y^2)$ are independent of each other and $S_i$. The estimand is $E[Y_{ij}(t+1)] - E[Y_{ij}(t)] = E[Y_i(t+1)\mid S_i=s] - E[Y_i(t)\mid S_i=s] = \beta_T$. Let $Y$ denote the vector of outcomes for all observations.

::: {.callout-note icon="false"}
#### Bias of $\tilde{\beta}_T$

Fit models on observed data $T_i\mid S_i \overset{\mathrm{iid}}{\sim} N(\tilde{f}(S_i), \tilde{\sigma}_T^2)$ and $Y\mid T,S \sim N(\tilde{\beta}_TT + \tilde{h}(S),\Sigma)$ with GP priors on $\tilde{f}$ and $\tilde{h}$. $\Sigma$ follows the block diagonal structure with $Var(Y_{ij} \mid T_i,S_i) = \tilde{\sigma}_Y^2$ and $Cov(Y_{ij},Y_{ik}\mid T, S) = \tilde{\sigma}_Y^2 - \sigma_Y^2$. Then $\tilde{\sigma}_T^2$, $\tilde{\sigma}_Y^2$ and $\sigma_Y^2$ are identifiable and the bias is:

$$
|\tilde{\beta}_T - \beta_T|^2 = E(Y-\tilde{g})^2E\tilde{\alpha}^2 \eta_{Y\sim U|T,X,S}^2 \frac{\eta_{T\sim U|X,S}^2}{1-\eta_{T\sim U|X,S}^2} = \frac{\tilde{\sigma}_Y^2 - \sigma_Y^2}{\tilde{\sigma}_T^2}\frac{\eta_{T\sim U \mid X,S}^2}{1-\eta_{T\sim U\mid X,S}^2}
$$
:::

Note that $\tilde{h}(S)$ are repeated measurements, so the GP prior should be assigned to non-repeated locations.

Another way of recovering $\sigma^2_Y$ is to fit a mixed effect model $Y_{ij} = \tilde{\beta}_TT_i + \tilde{h}(S_i) + \tilde{V}_i + \epsilon_{ij}$.

The idea about variables at different scale works for general hierarchical models [(Witty et al. 2020)](https://proceedings.mlr.press/v119/witty20a/witty20a.pdf). But this paper modeled entire treatment, outcome with GP.

#### Simulation

In the simulation, we generate data with $\beta_T = \beta_U =1, \sigma^2=\sigma_T^2=\sigma_Y^2=0.2$, so $\eta_{T\sim U\mid X,S}^2 =0.5, \tilde{\sigma}_T^2 = 0.4, \tilde{\sigma}_Y^2 = 0.3$. $f(S)$, $Z$ and $W$ are generated from GP. (If able to use sufficient statistics like average in location and variance can make computation easier.)

Follow the Bayesian workflow suggested in [Birthdays notebook](https://avehtari.github.io/casestudies/Birthdays/birthdays.html#Workflow_for_quick_iterative_model_building), we first use optimization and mcmc with small number of iterations for quick testing of the model. We check if $\tilde{\beta}_T$, $\tilde{\sigma}_T^2$, $\tilde{\sigma}_Y^2$ and $\sigma_Y^2$ can be consistently estimated by fitting models $T_i\mid S_i \overset{\mathrm{iid}}{\sim} N(\tilde{f}(S_i), \tilde{\sigma}_T^2)$ and $Y\mid T,S \sim N(\tilde{\beta}_TT + \tilde{h}(S),\Sigma)$ as above.
:::

```{r mcmc, message=FALSE}
# set.seed(111)
# estimate sigma_t_tilde
m <- 1000 # number of locations
s <- matrix(runif(2*m, -1, 1), ncol = 2)
u <- gp_sample(s, a2 = 1, l = 1, sigma2 = 0.2)
z <- gp_sample(s, a2 = 1, l = 2, sigma2 = 0.2)
t <- u + z # + rnorm(n, mean = 0, sd = sqrt(0.2))
data_list_t <- list(D=ncol(s), N=m, s=s[1:m,], t=t[1:m,])
trt_model <- cmdstan_model(stan_file = "stan/treatment_gp.stan")
trt_opt <- trt_model$optimize(data=data_list_t, init=2, refresh=1000, iter=10000)
opt_draws <- trt_opt$draws()
cat("MAP of sigma_t_tilde_sq =", as.numeric(subset(opt_draws, variable=c('sigma_t')))^2)

set.seed(123)
m <- 100 # number of locations
n <- 300
s <- matrix(runif(2*m, -1, 1), ncol = 2) # rep(s[,1], times=a)
u <- gp_sample(s, a2 = 1, l = 1, sigma2 = 0.2) 
z <- gp_sample(s, a2 = 1, l = 2, sigma2 = 0.2)
w <- gp_sample(s, a2 = 1, l = 3, sigma2 = 0)
t <- u + z # + rnorm(m, mean = 0, sd = sqrt(0.2))  
# s <- s[rep(1:nrow(s), each=n/m),] 
u <- u[rep(1:nrow(u), each=n/m),] 
t <- t[rep(1:nrow(t), each=n/m),] 
w <- w[rep(1:nrow(w), each=n/m),] 
# s <- do.call(rbind, replicate(5, s, simplify=FALSE))
# u <- do.call(rbind, replicate(5, u, simplify=FALSE))
# t <- do.call(rbind, replicate(5, t, simplify=FALSE))
y <- t + u + w + rnorm(n, mean = 0, sd = sqrt(0.2)) # + w

# plot
# u_density <- tibble(f = u, s1 = s[,1], s2 = s[,2])
# u_density %>%
#   ggplot(aes(x=s1, y=s2, color=f)) +
#   geom_point(alpha=1, size=2.5) +
#   theme_bw()

# # estimate sigma_y_tilde
# data_list_y <- list(D=ncol(s), N=length(y), s=s[rep(1:nrow(s), each=n/m),] , t=t, y=y)
# outcome_model <- cmdstan_model(stan_file = "outcome_gp.stan")
# outcome_opt <- outcome_model$optimize(data=data_list_y, init=2, refresh=100, iter=10000)
# opt_draws <- outcome_opt$draws()
# print(subset(opt_draws, variable=c('beta', 'sq_sigma_y')))

# MAP estimate of sigma_y_tilde and sigma_y
data_list <- list(D=ncol(s), M=m, N=length(y), s=s, t=t, y=y)
outcome_model_1 <- cmdstan_model(stan_file = "stan/replication_t_u.stan")
outcome_opt_1 <- outcome_model_1$optimize(data=data_list, init=2, refresh=500, iter=10000)
opt_draws <- outcome_opt_1$draws()
print(subset(opt_draws, variable=c('beta', 'sq_sigma_y', 'sq_sigma_y_tilde')))

# mcmc 
run_sampler <- 0
if(run_sampler) {
  stan_results <- outcome_model_1$sample(data_list, iter_warmup = 1000, 
                                         iter_sampling = 1000, refresh = 500) 
  stan_results$save_object(file="simulation_rep_n300.RDS")
} else {
  stan_results <- readRDS("simulation_rep_n300.RDS")
}
result <- stan_results$summary(variables = c('beta', 'sq_sigma_y', 'sq_sigma_y_tilde'))
print(result)
```

The posterior estimates are close to true values of $\tilde{\sigma}_T^2$, $\tilde{\beta}_T$, $\tilde{\sigma}_Y^2$ and $\sigma_Y^2$, and the $90\%$ credible intervals cover them.

Next we try faster approximate Bayesian inference using [R-INLA](https://www.r-inla.org/home). The continuous domain spatial modeling is done by stochastic partial differential equation (SPDE) approach. We first build a treatment model.

```{r INLA}
set.seed(123)
n <- 500 # number of locations
s <- matrix(runif(2*n, -1, 1), ncol = 2)
u <- gp_sample(s, a2 = 1, l = 1, sigma2 = 0.2)
z <- gp_sample(s, a2 = 1, l = 2, sigma2 = 0.2)
t <- u + z # + rnorm(n, mean = 0, sd = sqrt(0.2))
# data1 <- data.frame(s1 = s[,1], s2 = s[,2], t = t)
# plot
tibble(f = t, s1 = s[,1], s2 = s[,2]) %>%
  ggplot(aes(x=s1, y=s2, color=f)) +
  geom_point(alpha=1, size=2.5) +
  theme_bw()

# use fmesher to create meshes, tradeoff between accuracy and computational cost
mesh1 <- fm_mesh_2d_inla(loc=s, max.edge=c(0.05, 0.15), cutoff=0.02, offset=c(0.1,0.4))
plot(mesh1)
points(s, pch=3, bg=1, col="red", cex=1)

# create projector matrix A, mapping triangulation vertices on mesh to observation locations
A.est <- inla.spde.make.A(mesh=mesh1, loc=s)
print(dim(A.est)) # number of data locations * number of mesh nodes

# fit model
spde <- inla.spde2.matern(mesh=mesh1, alpha=2) 
formula <- y ~ -1 + intercept + f(spatial.field, model=spde)
# spatial.field is a index variable?
result <- inla(formula, 
               data = list(y=t[,1], intercept=rep(1, spde$n.spde),
               spatial.field=1:spde$n.spde),
               control.predictor=list(A=A.est, compute=TRUE))

# round(result$summary.fixed, 3)
print(round(result$summary.hyperpar, 3))
cat("posterior mean of sigma_t_tilde_sq =", inla.emarginal(function(x) 1/x, result$marginals.hyper[[1]])) # posterior mean of variance
```

Posterior mean is close to $0.4$ and $95\%$ credible interval covers true $\tilde{\sigma}_T^2$.

Then we check the posterior distributions of $\tilde{\beta}_T$, $\tilde{\sigma}_Y^2$ and $\sigma_Y^2$ by fitting $Y_{ij} = \beta_0 + \tilde{\beta}_TT_i + \tilde{h}(S_i) + \tilde{V}_i + \epsilon_{ij}$ without marginalizing out $U_i$. The random effect $\tilde{V}_i$ accounts for the between-group variation $\beta_U^2Var(U_i \mid T_i,S_i)= \tilde{\sigma}_Y^2 -\sigma_Y^2 =0.1$.

```{r replication}
# with replicates at each location
set.seed(111)
m <- 100 # number of locations
n <- 500
s <- matrix(runif(2*m, -1, 1), ncol = 2) # rep(s[,1], times=a)
group <- rep(1:m, each=n/m)
u <- gp_sample(s, a2 = 1, l = 1, sigma2 = 0.2) 
z <- gp_sample(s, a2 = 1, l = 2, sigma2 = 0.2)
w <- gp_sample(s, a2 = 1, l = 3, sigma2 = 0)
t <- u + z # + rnorm(m, mean = 0, sd = sqrt(0.2))  
u <- u[rep(1:nrow(u), each=n/m),] 
t <- t[rep(1:nrow(t), each=n/m),] 
w <- w[rep(1:nrow(w), each=n/m),] 
y <- t + u + w + rnorm(n, mean = 0, sd = sqrt(0.2)) 

mesh <- fm_mesh_2d_inla(loc=s, max.edge=c(0.05, 0.2), offset=c(0.1,0.4))

# For each observation, index gives the corresponding index into the matrix of measurement locations, and repl determines the corresponding replicate index. 
A <- inla.spde.make.A(mesh, loc = s, index = rep(1:m, each = n/m), 
                      repl = rep(1:(n/m), times = m))
print(dim(A))
spde <- inla.spde2.matern(mesh=mesh, alpha=2) 

# index the full mesh and replicates
mesh.index <- inla.spde.make.index(name = "field", n.spde = spde$n.spde,
                                   n.repl = n/m)

# stack the predictor information
stack <- inla.stack(data = list(y = y), A = list(A, 1),
                    effects = list(c(mesh.index, list(intercept = 1)),
                                   list(cov = t, loc = group)), tag = "est")

# fit model
formula <- y ~ -1 + intercept + cov + f(field, model = spde, replicate = field.repl) + f(loc, model = "iid")
rep_result <- inla(formula, data = inla.stack.data(stack, spde = spde),
                   family = "normal",
                   control.predictor = list(A = inla.stack.A(stack),
                                             compute = TRUE))
print(round(rep_result$summary.fixed, 3))
print(round(rep_result$summary.hyperpar, 3))
cat("posterior mean of sigma_y_tilde_sq - sigma_y_sq =", inla.emarginal(function(x) 1/x, rep_result$marginals.hyper[[4]]), '\n')
cat("posterior mean of sigma_y_sq =", inla.emarginal(function(x) 1/x, rep_result$marginals.hyper[[1]])) 

posterior_beta_tilde <- rep_result$marginals.fixed[[2]]
plot(posterior_beta_tilde, type = "l", xlab = 'beta_t_tilde', ylab = 'density',
     main = "Posterior density of beta_t_tilde")

posterior_sigma_y_sq <- rep_result$marginals.hyperpar[[1]]
posterior_sigma_y_sq[,1] <- 1 / posterior_sigma_y_sq[,1]
plot(posterior_sigma_y_sq, type = "l", xlab = 'sigma_y_sq', ylab = 'density',
     main = "Posterior density of sigma_y_sq")

posterior_sigma_y_tilde_sq <- rep_result$marginals.hyperpar[[4]]
posterior_sigma_y_tilde_sq[,1] <- 1 / posterior_sigma_y_tilde_sq[,1] +
  posterior_sigma_y_sq[,1]
plot(posterior_sigma_y_tilde_sq, type = "l", xlab = 'sigma_y_tilde_sq', ylab = 'density',
     main = "Posterior density of sigma_y_tilde_sq")
```

The $95\%$ credible intervals cover true $\tilde{\beta}_T, \tilde{\sigma}_Y^2-\sigma_Y^2, \sigma_Y^2$.




